# Warmup

Warmup as a method of training learning rate in neural network training normally starts with a relatively small learning rate. After training a few epochs or steps, then it will be able to increase the learning rate a bit, training again with the new learning rate until it keeps increasing and reaches the initial regular learning rate. It can keep growing until over the regular learning rate in some scenarios.

Warmup adopts this generally increasing method, mainly to prevent the instability of model. In the first instance, when the weight of the model is randomly initialised, using a relatively large learning rate straight away is likely to cause unstable model. However, a relatively small learning rate is able to help the model tend to become more stable which develops into a better model for neural network training.